# ğŸ–¼ï¸ DeepCIFAR-ImageClassifier

This project builds, trains, and improves a **Convolutional Neural Network (CNN)** for image classification on the **CIFAR-10 dataset** â€” a challenging dataset of 60,000 32x32 color images across 10 classes such as airplanes, cars, and birds.

The goal is to start with a baseline CNN and incrementally apply techniques to enhance accuracy and generalization.

---

## ğŸ—’ï¸ Table of Contents

- [ğŸ“Œ Problem Statement](#-problem-statement)  
- [ğŸ¯ Objectives](#-objectives)  
- [ğŸ§ª Methodology](#-methodology)  
- [ğŸ“ˆ Results](#-results)  
- [ğŸ§  Key Insights](#-key-insights)  
- [âš ï¸ Challenges](#-challenges)  
- [ğŸš€ Improvements](#-improvements)  
- [ğŸ“ Conclusion](#-conclusion)  
- [ğŸ› ï¸ Setup Instructions](#-setup-instructions)  

---

## ğŸ“Œ Problem Statement

Image classification remains a foundational problem in computer vision, with CIFAR-10 serving as a popular benchmark due to its complexityâ€”small 32x32 color images belonging to 10 diverse categories. The challenge lies in achieving high accuracy while avoiding overfitting given limited image resolution and dataset size.

---

## ğŸ¯ Objectives

- Develop a baseline CNN to classify CIFAR-10 images.  
- Experiment with architectural changes including dropout, batch normalization, and deeper layers.  
- Apply data augmentation to improve generalization.  
- Use learning rate scheduling and regularization to stabilize training.  
- Evaluate and analyze performance gains from each modification.

---

## ğŸ§ª Methodology

### ğŸ“ Data Loading & Preprocessing

- Dataset: CIFAR-10 via Keras API  
- Normalized pixel values to [0, 1]  
- One-hot encoded categorical labels  

### ğŸ§± Baseline CNN Architecture

| Layer Type       | Details                              |
|------------------|------------------------------------|
| Conv2D           | 2 layers with ReLU activations     |
| MaxPooling2D     | After each convolutional block      |
| Fully Connected  | Dense layers with ReLU activation  |
| Output           | Softmax layer with 10 units        |

### ğŸ§© Enhancements Explored

- Dropout for regularization  
- Batch Normalization to stabilize and speed up training  
- Adding more convolutional layers (deeper network)  
- Data augmentation (random flips, rotations)  
- Learning rate scheduling (Cosine Decay, ReduceLROnPlateau)  
- Optimizer upgrade to AdamW for weight decay  

---

## ğŸ“ˆ Results

| Model                                    | Test Accuracy | Notes                                            |
|------------------------------------------|---------------|-------------------------------------------------|
| Baseline CNN                             | 67.99%        | Overfitting evident, simple architecture         |
| Baseline + Dropout                       | 67.31%        | Overfitting reduced, slight accuracy drop        |
| Deeper Model + Batch Normalization      | 73.64%        | Improved accuracy and training stability         |
| Data Augmentation Only                   | 64.39%        | Better generalization, lower training accuracy   |
| Combined (Reg + Aug + BN)                | 65.94%        | Mixed effects, stronger regularization           |
| Best Model (Deep + Dropout + BN + AdamW)| **89.29%**   | Optimized architecture with major performance boost |

---

## ğŸ§  Key Insights

- Batch Normalization enabled deeper architectures by stabilizing gradients.  
- Dropout was crucial to reduce overfitting, especially in dense layers.  
- Data augmentation improved generalization but required careful tuning.  
- Learning rate schedulers helped maintain stable and efficient training.  
- AdamW optimizer with weight decay contributed significantly to final accuracy gains.

---

## âš ï¸ Challenges

- Managing overfitting while maintaining accuracy.  
- Finding the right balance for augmentation strategies to avoid hurting performance.  
- Computational cost of deeper models and longer training times.  
- Tuning hyperparameters like dropout rates, learning rates, and batch sizes.

---

## ğŸš€ Improvements

Future work could include:

- Transfer learning with pre-trained models like ResNet or EfficientNet.  
- Exploring more complex augmentation pipelines (Cutout, Mixup).  
- Hyperparameter optimization (automated tuning).  
- Experimenting with attention mechanisms and newer CNN architectures.

---

## ğŸ“ Conclusion

Incrementally improving the CNN by adding depth, batch normalization, dropout, and optimized training strategies resulted in a strong CIFAR-10 classifier with **89.29% test accuracy**. This highlights the importance of architectural choices combined with effective regularization and learning rate management.

---

## ğŸ› ï¸ Setup Instructions

```bash
   git clone https://github.com/yourusername/cnn-cifar10-image-classification.git
   cd cnn-cifar10-image-classification
```
---
## References
- CIFAR-10 Dataset: https://www.cs.toronto.edu/~kriz/cifar.html
- Keras Documentation: https://keras.io
- Batch Normalization Paper: Sergey Ioffe, Christian Szegedy, 2015
- AdamW Optimizer: Loshchilov and Hutter, 2017
